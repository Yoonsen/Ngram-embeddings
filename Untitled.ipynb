{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afd310ea-783b-46b4-833e-bbc86e33909f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.sparse import csr_matrix\n",
    "import warnings\n",
    "\n",
    "class BaseWordEmbedding:\n",
    "    def __init__(self, n_base_words=200, projection_type='random'):\n",
    "        \"\"\"\n",
    "        Initialize embedding system using either random projection\n",
    "        or explicit base word dimensions.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_base_words : int\n",
    "            Number of dimensions/base words to use\n",
    "        projection_type : str\n",
    "            'random' for standard random projection\n",
    "            'base_words' to use specific words as dimensions\n",
    "        \"\"\"\n",
    "        self.n_dimensions = n_base_words\n",
    "        self.projection_type = projection_type\n",
    "        self.projection_matrix = None\n",
    "        self.base_words = None\n",
    "        self.base_word_vectors = None\n",
    "        \n",
    "    def fit(self, dtm, words, base_words=None):\n",
    "        \"\"\"\n",
    "        Fit the projection system.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        dtm : scipy.sparse.csr_matrix\n",
    "            The input DTM with trigram-based word representations\n",
    "        words : list\n",
    "            List of words corresponding to DTM rows\n",
    "        base_words : list, optional\n",
    "            List of words to use as base dimensions. Required if \n",
    "            projection_type is 'base_words'\n",
    "        \"\"\"\n",
    "        if self.projection_type == 'random':\n",
    "            # Standard random projection matrix\n",
    "            input_dims = dtm.shape[1]\n",
    "            self.projection_matrix = np.random.normal(\n",
    "                0, 1/np.sqrt(self.n_dimensions), \n",
    "                (input_dims, self.n_dimensions)\n",
    "            )\n",
    "            \n",
    "        elif self.projection_type == 'base_words':\n",
    "            if base_words is None or len(base_words) != self.n_dimensions:\n",
    "                raise ValueError(\"Must provide exactly n_base_words base words\")\n",
    "                \n",
    "            # Store base words and their indices\n",
    "            self.base_words = base_words\n",
    "            word_to_idx = {word: idx for idx, word in enumerate(words)}\n",
    "            base_indices = [word_to_idx[word] for word in base_words]\n",
    "            \n",
    "            # Extract base word vectors\n",
    "            self.base_word_vectors = normalize(\n",
    "                dtm[base_indices].toarray(), \n",
    "                norm='l2', \n",
    "                axis=1\n",
    "            )\n",
    "            \n",
    "    def transform(self, vectors):\n",
    "        \"\"\"\n",
    "        Project vectors into the reduced space.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        vectors : scipy.sparse.csr_matrix or numpy.ndarray\n",
    "            Vectors to project\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        numpy.ndarray\n",
    "            Projected vectors\n",
    "        \"\"\"\n",
    "        if isinstance(vectors, csr_matrix):\n",
    "            vectors = vectors.toarray()\n",
    "            \n",
    "        if self.projection_type == 'random':\n",
    "            projected = vectors @ self.projection_matrix\n",
    "            \n",
    "        else:  # base_words\n",
    "            # Project onto base word vectors\n",
    "            projected = vectors @ self.base_word_vectors.T\n",
    "            \n",
    "        return normalize(projected, norm='l2', axis=1)\n",
    "    \n",
    "    def get_dimension_interpretation(self, n_contributing_words=5):\n",
    "        \"\"\"\n",
    "        For base word projection, returns the base word for each dimension.\n",
    "        For random projection, returns words that contribute most to each dimension.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        list of str or list of list of (str, float)\n",
    "            Interpretation of each dimension\n",
    "        \"\"\"\n",
    "        if self.projection_type == 'base_words':\n",
    "            return self.base_words\n",
    "        else:\n",
    "            # For random projection, find words that align most with each dimension\n",
    "            interpretations = []\n",
    "            for dim_idx in range(self.n_dimensions):\n",
    "                dim_weights = self.projection_matrix[:, dim_idx]\n",
    "                top_indices = np.argsort(np.abs(dim_weights))[-n_contributing_words:]\n",
    "                weights = dim_weights[top_indices]\n",
    "                interpretations.append([\n",
    "                    (f\"Feature_{i}\", float(w))\n",
    "                    for i, w in zip(top_indices, weights)\n",
    "                ])\n",
    "            return interpretations\n",
    "\n",
    "    def interpret_embedding(self, embedding, n_top=5):\n",
    "        \"\"\"\n",
    "        Interpret what a given embedding represents in terms of\n",
    "        base dimensions or random projection components.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        embedding : numpy.ndarray\n",
    "            The embedding vector to interpret\n",
    "        n_top : int\n",
    "            Number of top contributing dimensions to return\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        list of tuple\n",
    "            (dimension_name, contribution) pairs\n",
    "        \"\"\"\n",
    "        # Get absolute contributions\n",
    "        contributions = np.abs(embedding)\n",
    "        top_indices = np.argsort(contributions)[-n_top:][::-1]\n",
    "        \n",
    "        if self.projection_type == 'base_words':\n",
    "            return [(self.base_words[idx], float(embedding[idx]))\n",
    "                    for idx in top_indices]\n",
    "        else:\n",
    "            return [(f\"Dimension_{idx}\", float(embedding[idx]))\n",
    "                    for idx in top_indices]\n",
    "\n",
    "# Example usage\n",
    "def demonstrate_usage():\n",
    "    # Sample data\n",
    "    vocabulary = [\"cat\", \"dog\", \"fish\", \"bird\", \"hamster\"]\n",
    "    dtm = np.random.rand(5, 1000)  # 5 words, 1000 trigram features\n",
    "    dtm_sparse = csr_matrix(dtm)\n",
    "    \n",
    "    # Random projection\n",
    "    random_embedder = BaseWordEmbedding(n_base_words=3, projection_type='random')\n",
    "    random_embedder.fit(dtm_sparse, vocabulary)\n",
    "    random_embeddings = random_embedder.transform(dtm_sparse)\n",
    "    \n",
    "    # Base word projection\n",
    "    base_embedder = BaseWordEmbedding(n_base_words=3, projection_type='base_words')\n",
    "    base_embedder.fit(dtm_sparse, vocabulary, base_words=['cat', 'dog', 'fish'])\n",
    "    base_embeddings = base_embedder.transform(dtm_sparse)\n",
    "    \n",
    "    return random_embeddings, base_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a71fe4-fd32-4cc0-aa54-b780ef1373a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
